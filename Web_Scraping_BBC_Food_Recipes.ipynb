{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import urllib.request\n",
    "import re\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import requests\n",
    "\n",
    "\n",
    "BBC_MAIN_URL = 'https://www.bbc.co.uk/'\n",
    "current_folder = os.getcwd()\n",
    "\n",
    "\n",
    "def make_directories(folder):\n",
    "    #make a new directory to save the images\n",
    "    if not os.path.exists(folder):\n",
    "        os.makedirs(folder)\n",
    "        \n",
    "def dataset(index_images_list, categories, names_recipes_list, links_to_recipes, ingredients_plus_quantities, methods, chef_list, images_yes_no,alphabetic_letter, newpathcsv):    \n",
    "    list_all = []\n",
    "    for i in range(len(links_to_recipes)):\n",
    "        \n",
    "        \n",
    "        new_ls = [categories[i], names_recipes_list[i], links_to_recipes[i] , '; '.join(ingredients_plus_quantities[i]), methods[i], chef_list[i], images_yes_no[i], index_images_list[i]]\n",
    "        list_all.append(new_ls)\n",
    "    data = pd.DataFrame(list_all, columns = ['category', 'name_recipe', 'links', 'ingred_and_quant', 'descr_method', 'chef', 'images_yes_no', 'index_images'])\n",
    "    name_dataset = 'dataset_'+alphabetic_letter.upper()+'.csv'\n",
    "    data = data.to_csv(os.path.join(newpathcsv, name_dataset), index = True)\n",
    "\n",
    "\n",
    "def web_scraper_bbc_food(letter = 'a'):\n",
    "    \n",
    "    newpathcsv = os.path.join(current_folder,'csv_data')\n",
    "    newpathimages = os.path.join(current_folder,'images')\n",
    "    make_directories(newpathcsv)\n",
    "    make_directories(newpathimages)\n",
    "    \n",
    "    #homepage with the indexes pages to find the recipes https://www.bbc.co.uk/food/recipes/a-z/a/1#featured-content\n",
    "    url_initial_page = ''.join((BBC_MAIN_URL,'/food/recipes/a-z/', letter ,'/1#featured-content'))\n",
    "    headers = {'connection':'close','user-agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/86.0.4240.75 Safari/537.36'}\n",
    "    \n",
    "    time.sleep(3)\n",
    "\n",
    "    reqpage = urllib.request.Request(url_initial_page, headers = headers)\n",
    "    respagep = urllib.request.urlopen(reqpage)\n",
    "    respagepData = respagep.read()\n",
    "\n",
    "\n",
    "    #alphabetic links at the top page\n",
    "    links_alphabetic = re.findall(r'<ul class=\"az-keyboard__list\">(.*?)</ul>',  respagepData.decode('UTF-8'))\n",
    "    links_alphabetic = re.findall(r'<a class=.*? href=(.*?)>', str(links_alphabetic))\n",
    "    \n",
    "    \n",
    "    #links = ['/food/recipes/a-z/0-9/1#featured-content']\n",
    "    links_top_page = []\n",
    "    for i in links_alphabetic:\n",
    "        links_from_letter = ''.join(('/food/recipes/a-z/', letter ,'/1#featured-content'))\n",
    "        if i[1:-1] >= links_from_letter:\n",
    "            links_top_page.append(i)\n",
    "    \n",
    "    print(links_top_page)\n",
    "    count_recipes_w_images = 0\n",
    "    count_pages = 1\n",
    "    for link_top_page in links_top_page:\n",
    "        \n",
    "        lk = link_top_page[1:-1]\n",
    "        url = ''.join((BBC_MAIN_URL,lk))\n",
    "        print(url, 'url')\n",
    "        headers = {'connection':'close','user-agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/86.0.4240.75 Safari/537.36'}\n",
    "        time.sleep(3)\n",
    "\n",
    "\n",
    "        alphabetic_letter = re.findall(r'/food/recipes/a-z/(.*?)/[0-9]+#', lk)\n",
    "        page = urllib.request.Request(url,headers = headers)\n",
    "        reqpage = urllib.request.urlopen(page)\n",
    "        reqpagepData = reqpage.read()\n",
    "\n",
    "        #links at the bottom page \n",
    "        links_on_index = re.findall(r'<a class=\"pagination__link gel-pica-bold\" href=(.*?)>', reqpagepData.decode('UTF-8'))\n",
    "\n",
    "\n",
    "        #find the maxiumn number of pages in the alphabetic section of each main pages\n",
    "        num_pages = []\n",
    "        for i in links_on_index:\n",
    "            num = re.findall(r'([0-9]+)#', i)\n",
    "            num_pages.extend(num)\n",
    "            \n",
    "        if len(num_pages) == 0:\n",
    "            num_max_index = 1\n",
    "        else:\n",
    "            num_max_index = int(max(num_pages))\n",
    "\n",
    "\n",
    "        index_images_list, categories, names_recipes_list, links_to_recipes, ingredients_plus_quantities, methods, chef_list, images_yes_no, = ([]for i in range(8))\n",
    "\n",
    "        for index_link in range(1,num_max_index+1):\n",
    "            print(alphabetic_letter,'alphabetic_letter')\n",
    "            url = ''.join((BBC_MAIN_URL,\"food/recipes/a-z/\"+alphabetic_letter[0]+\"/\"+str(index_link)+\"#featured-content\"))\n",
    "            headers = {'connection':'close','user-agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/86.0.4240.75 Safari/537.36'}\n",
    "\n",
    "            time.sleep(3)\n",
    "\n",
    "            page_index = urllib.request.Request(url,headers = headers)\n",
    "            reqpage_index = urllib.request.urlopen(page_index)\n",
    "            reqpageData_idex = reqpage_index.read()\n",
    "\n",
    "             #links to the recipes\n",
    "            links_on_main_page = re.findall(r'<div class=\"gel-layout__item gel-1/2 gel-1/3@m gel-1/4@xl\">(.*?)</div>', reqpageData_idex.decode('UTF-8'))\n",
    "            links_on_main_page = list(set(links_on_main_page))\n",
    "\n",
    "\n",
    "            #extracts links to recipes and their food categories\n",
    "            links_and_categories = []\n",
    "            for link in links_on_main_page:\n",
    "                link_temp = re.findall(r'<a class=\"promo promo__(.*?)\" href=(.*?)>', link)\n",
    "                links_and_categories.extend(link_temp)\n",
    "\n",
    "            for category, link in links_and_categories:\n",
    "                \n",
    "                #separate in tuples the caterigories of the food from their web page links\n",
    "                link_bbc = ''.join((BBC_MAIN_URL, link[2:-1]))\n",
    "                categories.append(category)\n",
    "                links_to_recipes.append(link_bbc)\n",
    "                \n",
    "\n",
    "                url = link_bbc\n",
    "                headers = {'connection':'close','user-agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/86.0.4240.75 Safari/537.36'}\n",
    "\n",
    "                time.sleep(3)\n",
    "\n",
    "                req = urllib.request.Request(url, headers = headers)\n",
    "                resp = urllib.request.urlopen(req)\n",
    "                respData = resp.read()\n",
    "\n",
    "                #finds the chef or author of the recipe\n",
    "                chef = re.findall(r'<a class=\"chef__link\" href=\"/food/chefs/.*?\">(.*?)</a>', respData.decode('UTF-8'))\n",
    "                chef_list.append(chef[0])\n",
    "\n",
    "                #find image if any\n",
    "                image = re.findall(r'<div class=\"recipe-media__image responsive-image-container__16/9\">(.*?)</div>', respData.decode('UTF-8'))\n",
    "                image_links = re.findall(r'<.*?src=\"(.*?)\"', str(image))\n",
    "\n",
    "                #name recipe\n",
    "                name = re.findall(r'<h1 class=\"gel-trafalgar content-title__text\">(.*?)</h1>',respData.decode('UTF-8') )\n",
    "                names_recipes_list.append(name[0])\n",
    "\n",
    "                #description of the method to prepare the recipes\n",
    "                method = re.findall(r'<ol class=\"recipe-method__list\">(.*?)</ol>',  respData.decode('UTF-8'))\n",
    "                method = re.sub(r'<.*?>|\\\\',\"\",str(method)).strip()\n",
    "                methods.append(method[1:-1])\n",
    "\n",
    "                #search for the ingredients\n",
    "                ingr = re.findall('<ul class=\"recipe-ingredients__list\">(.*?)</ul>', respData.decode('UTF-8'))\n",
    "                ingred = re.findall(r'.+>(.*?)<.+', str(ingr))\n",
    "                ingredients = re.findall('<li class=\"recipe-ingredients__list-item\"> *(.*?)</li>', str(ingr))#just ingredients\n",
    "                \n",
    "                \n",
    "                ing = []\n",
    "                for i in ingredients:\n",
    "                    ingred = re.sub(r'<.*?>',\"\",str(i).strip())\n",
    "                    ing.append(ingred)\n",
    "                ingredients_plus_quantities.append(ing)\n",
    "\n",
    "                if len(image_links) != 0:\n",
    "\n",
    "                    # set filename and image URL\n",
    "                    index_name_image = ''.join((alphabetic_letter[0], str(count_recipes_w_images)))\n",
    "                    filename = 'image_'+ index_name_image +'.jpg'\n",
    "                    filename_path = os.path.join(newpathimages, filename)\n",
    "                    image_url = image_links[0]\n",
    "\n",
    "                    # call urlretrieve function to download image\n",
    "                    urllib.request.urlretrieve(image_url, filename_path)\n",
    "                    images_yes_no.append('yes')\n",
    "                    index_images_list.append(index_name_image)\n",
    "                else:\n",
    "                    images_yes_no.append('no')\n",
    "                    index_images_list.append('')\n",
    "\n",
    "                count_recipes_w_images +=1\n",
    "        count_pages +=1\n",
    "        dataset(index_images_list, categories, names_recipes_list, links_to_recipes, ingredients_plus_quantities, methods, chef_list, images_yes_no, alphabetic_letter[0],newpathcsv)\n",
    "\n",
    "\n",
    "web_scraper_bbc_food('a')    \n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
