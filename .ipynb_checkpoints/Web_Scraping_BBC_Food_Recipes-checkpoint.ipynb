{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import urllib.request\n",
    "import re\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import requests\n",
    "\n",
    "\n",
    "BBC_MAIN_URL = 'https://www.bbc.co.uk/'\n",
    "\n",
    "#set a folder to save the images\n",
    "current_folder = os.getcwd()\n",
    "\n",
    "#make a new directory to save the images\n",
    "newpathimages = os.path.join(current_folder,'images')\n",
    "if not os.path.exists(newpathimages):\n",
    "    os.makedirs(newpathimages)\n",
    "\n",
    "#make a new directory to save the datasets\n",
    "newpathcsv = os.path.join(current_folder,'csv_data')\n",
    "if not os.path.exists(newpathcsv):\n",
    "    os.makedirs(newpathcsv)\n",
    "\n",
    "#homepage with the indexes pages to find the recipes\n",
    "url_initial_page = ''.join((BBC_MAIN_URL,'/food/recipes/a-z/a/1#featured-content'))\n",
    "headers = {'connection':'close','user-agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/86.0.4240.75 Safari/537.36'}\n",
    "\n",
    "time.sleep(3)\n",
    "\n",
    "reqpage = urllib.request.Request(url_initial_page, headers = headers)\n",
    "respagep = urllib.request.urlopen(reqpage)\n",
    "respagepData = respagep.read()\n",
    "\n",
    "\n",
    "#alphabetic links at the top page\n",
    "links_alphabetic = re.findall(r'<ul class=\"az-keyboard__list\">(.*?)</ul>',  respagepData.decode('UTF-8'))\n",
    "links_alphabetic = re.findall(r'<a class=.*? href=(.*?)>', str(links_alphabetic))\n",
    "\n",
    "\n",
    "count_recipes_w_images = 1\n",
    "count_pages = 1\n",
    "for link in links_alphabetic:\n",
    "    lk = link[1:-1]\n",
    "    url = ''.join((BBC_MAIN_URL,lk))\n",
    "    headers = {'connection':'close','user-agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/86.0.4240.75 Safari/537.36'}\n",
    "    time.sleep(3)\n",
    "    \n",
    "    \n",
    "    alphabetic_letter = re.findall(r'/food/recipes/a-z/(.*?)/[0-9]+#', lk)\n",
    "    \n",
    "    page = urllib.request.Request(url,headers = headers)\n",
    "    reqpage = urllib.request.urlopen(page)\n",
    "    reqpagepData = reqpage.read()\n",
    "    \n",
    "    #links at the bottom page \n",
    "    links_on_index = re.findall(r'<a class=\"pagination__link gel-pica-bold\" href=(.*?)>', reqpagepData.decode('UTF-8'))\n",
    "    \n",
    "    \n",
    "    #find the maxiumn number of pages in the alphabetic section of each main pages\n",
    "    num_pages = []\n",
    "    for i in links_on_index:\n",
    "        num = re.findall(r'([0-9]+)#', i)\n",
    "        num_pages.extend(num)\n",
    "        num_max_index = int(max(num_pages))\n",
    "    \n",
    "    \n",
    "    index_images_list, categories, names_recipes_list, links_to_recipes, ingredients_only, ingredients_plus_quantities, methods, chef_list, images_yes_no, = ([]for i in range(9))\n",
    "    \n",
    "    for index_link in range(1,num_max_index+1):\n",
    "        \n",
    "        url = ''.join((BBC_MAIN_URL,\"food/recipes/a-z/\"+alphabetic_letter[0]+\"/\"+str(index_link)+\"#featured-content\"))\n",
    "        headers = {'connection':'close','user-agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/86.0.4240.75 Safari/537.36'}\n",
    "\n",
    "        time.sleep(3)\n",
    "\n",
    "        page_index = urllib.request.Request(url,headers = headers)\n",
    "        reqpage_index = urllib.request.urlopen(page_index)\n",
    "        reqpageData_idex = reqpage_index.read()\n",
    "        \n",
    "         #links to the recipes\n",
    "        links_on_main_page = re.findall(r'<div class=\"gel-layout__item gel-1/2 gel-1/3@m gel-1/4@xl\">(.*?)</div>', reqpageData_idex.decode('UTF-8'))\n",
    "        links_on_main_page = list(set(links_on_main_page))\n",
    "        \n",
    "\n",
    "        #extracts links to recipes and their food categories\n",
    "        links_and_categories = []\n",
    "        for link in links_on_main_page:\n",
    "            link_temp = re.findall(r'<a class=\"promo promo__(.*?)\" href=(.*?)>', link)\n",
    "            links_and_categories.extend(link_temp)\n",
    "            \n",
    "           \n",
    "        for category, link in links_and_categories:\n",
    "            \n",
    "            #separate in tuples the caterigories of the food from their web page links\n",
    "            link_bbc = ''.join((BBC_MAIN_URL, link[2:-1]))\n",
    "            categories.append(category)\n",
    "            links_to_recipes.append(link_bbc)\n",
    "            \n",
    "            url = link_bbc\n",
    "            headers = {'connection':'close','user-agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/86.0.4240.75 Safari/537.36'}\n",
    "\n",
    "            time.sleep(3)\n",
    "\n",
    "            req = urllib.request.Request(url, headers = headers)\n",
    "            resp = urllib.request.urlopen(req)\n",
    "            respData = resp.read()\n",
    "            \n",
    "            #finds the chef or author of the recipe\n",
    "            chef = re.findall(r'<a class=\"chef__link\" href=\"/food/chefs/.*?\">(.*?)</a>', respData.decode('UTF-8'))\n",
    "            chef_list.append(chef[0])\n",
    "            \n",
    "            #find image if any\n",
    "            image = re.findall(r'<div class=\"recipe-media__image responsive-image-container__16/9\">(.*?)</div>', respData.decode('UTF-8'))\n",
    "            image_links = re.findall(r'<.*?src=\"(.*?)\"', str(image))\n",
    "    \n",
    "            #name recipe\n",
    "            name = re.findall(r'<h1 class=\"gel-trafalgar content-title__text\">(.*?)</h1>',respData.decode('UTF-8') )\n",
    "            names_recipes_list.append(name[0])\n",
    "        \n",
    "            #descrption of the method to prepare the recipes\n",
    "            method = re.findall(r'<ol class=\"recipe-method__list\">(.*?)</ol>',  respData.decode('UTF-8'))\n",
    "            method = re.sub(r'<.*?>|\\\\',\"\",str(method)).strip()\n",
    "            methods.append(method[1:-1])\n",
    "            \n",
    "            #search for the ingredients and the quantaties of each of them\n",
    "            ingredients_html = re.findall('<div class=\"recipe-ingredients-wrapper\">(.*?)</div>', respData.decode('UTF-8'))\n",
    "            ingredients = re.findall('li class=\"recipe-ingredients__list-item\">(.*?)<.*? class=\"recipe-ingredients__link\">(.*?)<.*?>(.*?)<', str(ingredients_html))#just ingredients\n",
    "            \n",
    "            quant_ingr = []\n",
    "            ingredient = []\n",
    "            for quant, ingr, extras in ingredients:\n",
    "                quant_ingr.append(''.join((quant, ingr, extras)))\n",
    "                ingredient.append(ingr)\n",
    "            ingredients_plus_quantities.extend(quant_ingr)\n",
    "            ingredients_only.extend(ingredient)\n",
    "            \n",
    "            if len(image_links) != 0:\n",
    "\n",
    "                # set filename and image URL\n",
    "                index_name_image = ''.join((alphabetic_letter[0], str(count_recipes_w_images)))\n",
    "                filename = 'image_'+ index_name_image +'.jpg'\n",
    "                filename_path = os.path.join(newpathimages, filename)\n",
    "                image_url = image_links[0]\n",
    "\n",
    "                # call urlretrieve function to download image\n",
    "                urllib.request.urlretrieve(image_url, filename_path)\n",
    "                images_yes_no.append('yes')\n",
    "                index_images_list.append(index_name_image)\n",
    "            else:\n",
    "                images_yes_no.append('no')\n",
    "                index_images_list.append('')\n",
    "            \n",
    "            count_recipes_w_images +=1\n",
    "    count_pages +=1\n",
    "    \n",
    "    list_all = []\n",
    "    for i in range(len(links_to_recipes)):\n",
    "        new_ls = [categories[i], names_recipes_list[i], links_to_recipes[i] , '; '.join(ingredients_only[i]) , '; '.join(ingredients_plus_quantities[i]), methods[i], chef_list[i], images_yes_no[i], index_images_list[i]]\n",
    "        list_all.append(new_ls)\n",
    "    data = pd.DataFrame(list_all, columns = ['category', 'name_recipe', 'links', 'ingredients', 'ingred_and_quant', 'descr_method', 'chef', 'images_yes_no', 'index_images'])\n",
    "    name_dataset = 'dataset'+str(count_pages)+'.csv'\n",
    "    data = data.to_csv(os.path.join(newpathcsv, name_dataset), index = True)\n",
    "    \n",
    "\n",
    "\n",
    "    \n",
    "    \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
